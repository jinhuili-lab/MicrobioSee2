<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd"><html xmlns="http://www.w3.org/1999/xhtml"><head><title>R: Compare performance of different models</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link rel="stylesheet" type="text/css" href="R.css" />
</head><body><div class="container">

<table width="100%" summary="page for compare_performance {performance}"><tr><td>compare_performance {performance}</td><td style="text-align: right;">R Documentation</td></tr></table>

<h2>Compare performance of different models</h2>

<h3>Description</h3>

<p><code>compare_performance()</code> computes indices of model
performance for different models at once and hence allows comparison of
indices across models.
</p>


<h3>Usage</h3>

<pre>
compare_performance(
  ...,
  metrics = "all",
  rank = FALSE,
  estimator = "ML",
  verbose = TRUE
)
</pre>


<h3>Arguments</h3>

<table summary="R argblock">
<tr valign="top"><td><code>...</code></td>
<td>
<p>Multiple model objects (also of different classes).</p>
</td></tr>
<tr valign="top"><td><code>metrics</code></td>
<td>
<p>Can be <code>"all"</code>, <code>"common"</code> or a character vector of
metrics to be computed. See related
<code><a href="../../performance/help/model_performance.html">documentation()</a></code> of object's class for
details.</p>
</td></tr>
<tr valign="top"><td><code>rank</code></td>
<td>
<p>Logical, if <code>TRUE</code>, models are ranked according to 'best'
overall model performance. See 'Details'.</p>
</td></tr>
<tr valign="top"><td><code>estimator</code></td>
<td>
<p>Only for linear models. Corresponds to the different
estimators for the standard deviation of the errors. If <code>estimator = "ML"</code>
(default, except for <code>performance_aic()</code> when the model object is of class
<code>lmerMod</code>), the scaling is done by <code>n</code> (the biased ML estimator), which is
then equivalent to using <code>AIC(logLik())</code>. Setting it to <code>"REML"</code> will give
the same results as <code>AIC(logLik(..., REML = TRUE))</code>.</p>
</td></tr>
<tr valign="top"><td><code>verbose</code></td>
<td>
<p>Toggle warnings.</p>
</td></tr>
</table>


<h3>Details</h3>



<h4>Model Weights</h4>

<p>When information criteria (IC) are requested in <code>metrics</code> (i.e., any of <code>"all"</code>,
<code>"common"</code>, <code>"AIC"</code>, <code>"AICc"</code>, <code>"BIC"</code>, <code>"WAIC"</code>, or <code>"LOOIC"</code>), model
weights based on these criteria are also computed. For all IC except LOOIC,
weights are computed as <code>w = exp(-0.5 * delta_ic) / sum(exp(-0.5 * delta_ic))</code>,
where <code>delta_ic</code> is the difference between the model's IC value and the
smallest IC value in the model set (Burnham and Anderson, 2002).
For LOOIC, weights are computed as &quot;stacking weights&quot; using
<code><a href="../../loo/help/loo_model_weights.html">loo::stacking_weights()</a></code>.
</p>



<h4>Ranking Models</h4>

<p>When <code>rank = TRUE</code>, a new column <code>Performance_Score</code> is returned.
This score ranges from 0\
performance. Note that all score value do not necessarily sum up to 100\
Rather, calculation is based on normalizing all indices (i.e. rescaling
them to a range from 0 to 1), and taking the mean value of all indices for
each model. This is a rather quick heuristic, but might be helpful as
exploratory index.
<br /> <br />
In particular when models are of different types (e.g. mixed models,
classical linear models, logistic regression, ...), not all indices will be
computed for each model. In case where an index can't be calculated for a
specific model type, this model gets an <code>NA</code> value. All indices that
have any <code>NA</code>s are excluded from calculating the performance score.
<br /> <br />
There is a <code>plot()</code>-method for <code>compare_performance()</code>,
which creates a &quot;spiderweb&quot; plot, where the different indices are
normalized and larger values indicate better model performance.
Hence, points closer to the center indicate worse fit indices
(see <a href="https://easystats.github.io/see/articles/performance.html">online-documentation</a>
for more details).
</p>



<h4>REML versus ML estimator</h4>

<p>By default, <code>estimator = "ML"</code>, which means that values from information
criteria (AIC, AICc, BIC) for specific model classes (like models from <em>lme4</em>)
are based on the ML-estimator, while the default behaviour of <code>AIC()</code> for
such classes is setting <code>REML = TRUE</code>. This default is intentional, because
comparing information criteria based on REML fits is usually not valid
(it might be useful, though, if all models share the same fixed effects -
however, this is usually not the case for nested models, which is a
prerequisite for the LRT). Set <code>estimator = "REML"</code> explicitly return the
same (AIC/...) values as from the defaults in <code>AIC.merMod()</code>.
</p>



<h3>Value</h3>

<p>A data frame with one row per model and one column per &quot;index&quot; (see
<code>metrics</code>).
</p>


<h3>Note</h3>

<p>There is also a <a href="https://easystats.github.io/see/articles/performance.html"><code>plot()</code>-method</a> implemented in the <a href="https://easystats.github.io/see/"><span class="pkg">see</span>-package</a>.
</p>


<h3>References</h3>

<p>Burnham, K. P., and Anderson, D. R. (2002).
<em>Model selection and multimodel inference: A practical information-theoretic approach</em> (2nd ed.).
Springer-Verlag. doi: <a href="https://doi.org/10.1007/b97636">10.1007/b97636</a>
</p>


<h3>Examples</h3>

<pre>

data(iris)
lm1 &lt;- lm(Sepal.Length ~ Species, data = iris)
lm2 &lt;- lm(Sepal.Length ~ Species + Petal.Length, data = iris)
lm3 &lt;- lm(Sepal.Length ~ Species * Petal.Length, data = iris)
compare_performance(lm1, lm2, lm3)
compare_performance(lm1, lm2, lm3, rank = TRUE)

m1 &lt;- lm(mpg ~ wt + cyl, data = mtcars)
m2 &lt;- glm(vs ~ wt + mpg, data = mtcars, family = "binomial")
m3 &lt;- lme4::lmer(Petal.Length ~ Sepal.Length + (1 | Species), data = iris)
compare_performance(m1, m2, m3)

</pre>

<hr /><div style="text-align: center;">[Package <em>performance</em> version 0.11.0 <a href="00Index.html">Index</a>]</div>
</div></body></html>
